{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Case Study: Cascading Models (Regression \u2192 Classification)\n",
                "\n",
                "## 1. The Concept: What is Cascading?\n",
                "In complex ML systems, we often chain models together. A common pattern is **Cascading**, where the *output* of Model A becomes an *input feature* for Model B.\n",
                "\n",
                "### Why do this?\n",
                "Sometimes, an intermediate variable is easier to predict than the final target, but that intermediate variable is highly correlated with the final target.\n",
                "\n",
                "**Scenario**: Real Estate Flipping.\n",
                "*   **Goal**: Predict *\"Is this house a good investment?\"* (Binary: Yes/No).\n",
                "*   **Problem**: \"Good investment\" depends heavily on the *Predicted Sale Price* vs *Current Listing Price*.\n",
                "*   **Solution**:\n",
                "    1.  **Model 1 (Regression)**: Predict the true market value of the house.\n",
                "    2.  **Feature Engineering**: Calculate `Profit_Margin = Predicted_Value - Listing_Price`.\n",
                "    3.  **Model 2 (Classification)**: Use `Profit_Margin` (and other features) to classify \"Invest\" or \"Pass\".\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Implementation: Diamond Premium Prediction\n",
                "We will use the **Diamonds** dataset.\n",
                "*   **Model A (Regression)**: Predict the fair price of a diamond based on its physical specs (`carat`, `depth`, `table`).\n",
                "*   **Model B (Classification)**: Predict if a diamond is a \"Premium Cut\" based on its specs AND its predicted fair price.\n",
                "    *   *Hypothesis*: Maybe Premium cuts are overpriced or underpriced in specific ways that the regression model captures?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, confusion_matrix\n",
                "\n",
                "# 1. Load Data\n",
                "# We drop missing values and sample for speed\n",
                "df = sns.load_dataset('diamonds').dropna().sample(2000, random_state=42)\n",
                "\n",
                "# 2. Define Targets\n",
                "# Regression Target: Price\n",
                "y_reg = df['price']\n",
                "# Classification Target: Is it a 'Premium' or 'Ideal' cut? (1=Yes, 0=No)\n",
                "y_class = df['cut'].apply(lambda x: 1 if x in ['Premium', 'Ideal'] else 0)\n",
                "\n",
                "# Features for Regression (Physical Specs)\n",
                "X_reg = df[['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
                "\n",
                "# Split Data (Must keep indices aligned!)\n",
                "X_train, X_test, y_reg_train, y_reg_test, y_class_train, y_class_test = train_test_split(\n",
                "    X_reg, y_reg, y_class, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(\"Data Splitting Complete.\")\n",
                "print(f\"Training Samples: {len(X_train)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Step 1: Train Regression Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "regressor = LinearRegression()\n",
                "regressor.fit(X_train, y_reg_train)\n",
                "\n",
                "# Generate Predictions on BOTH Train and Test steps\n",
                "# We need these predictions to train Model 2\n",
                "price_pred_train = regressor.predict(X_train)\n",
                "price_pred_test = regressor.predict(X_test)\n",
                "\n",
                "print(f\"Regression R2 Score: {regressor.score(X_test, y_reg_test):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Step 2: Feature Engineering (The Cascade)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We create new feature sets for the classifier\n",
                "X_class_train = X_train.copy()\n",
                "X_class_test = X_test.copy()\n",
                "\n",
                "# ADD the regression output as a feature\n",
                "X_class_train['Predicted_Fair_Price'] = price_pred_train\n",
                "X_class_test['Predicted_Fair_Price'] = price_pred_test\n",
                "\n",
                "# Check the new feature matrix\n",
                "display(X_class_train.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Step 3: Train Classification Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "classifier = LogisticRegression(max_iter=1000)\n",
                "classifier.fit(X_class_train, y_class_train)\n",
                "\n",
                "# Evaluate\n",
                "acc = accuracy_score(y_class_test, classifier.predict(X_class_test))\n",
                "print(f\"Classification Accuracy (Using Cascaded Pipe): {acc:.4f}\")\n",
                "\n",
                "# Compare with Baseline (Without the Price Feature)\n",
                "classifier_base = LogisticRegression(max_iter=1000)\n",
                "classifier_base.fit(X_train, y_class_train) # Using original X_train without price\n",
                "acc_base = accuracy_score(y_class_test, classifier_base.predict(X_test))\n",
                "\n",
                "print(f\"Baseline Accuracy (Without Price Feature):     {acc_base:.4f}\")\n",
                "print(f\"Improvement from Cascading: {(acc - acc_base)*100:.2f}%\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}