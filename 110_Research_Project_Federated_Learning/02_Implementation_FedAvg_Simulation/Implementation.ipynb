{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Comprehensive Research: Federated Learning (FedAvg)\n",
                "\n",
                "## 1. Concept Simulation\n",
                "**Objective**: Train a model on distributed data without data ever leaving the client.\n",
                "**Challenge**: Non-IID Data Distribution (Client A has different data than Client B).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import copy\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "\n",
                "# --- 1. FEDERATED DATA GENERATION ---\n",
                "# Create Global Dataset\n",
                "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
                "\n",
                "# Split into Client Data Islands (Non-IID)\n",
                "# Client A: Mostly Class 0\n",
                "# Client B: Mostly Class 1\n",
                "idx_0 = np.where(y == 0)[0]\n",
                "idx_1 = np.where(y == 1)[0]\n",
                "\n",
                "client_A_idx = np.concatenate([idx_0[:400], idx_1[:50]]) # 89% Class 0\n",
                "client_B_idx = np.concatenate([idx_0[400:450], idx_1[50:450]]) # 89% Class 1\n",
                "\n",
                "X_A, y_A = X[client_A_idx], y[client_A_idx]\n",
                "X_B, y_B = X[client_B_idx], y[client_B_idx]\n",
                "\n",
                "print(f\"Client A: {len(y_A)} samples. Class 0 Ratio: {(y_A==0).mean():.2%}\")\n",
                "print(f\"Client B: {len(y_B)} samples. Class 1 Ratio: {(y_B==1).mean():.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Infrastructure (Server & Clients)\n",
                "Since Scikit-Learn doesn't support manual weight injection easily, we implement a micro Neural Network using Numpy to demonstrate the **Math of Averaging**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleNN:\n",
                "    def __init__(self):\n",
                "        # Weights: 20 inputs -> 1 output\n",
                "        self.W = np.random.randn(20, 1) * 0.01\n",
                "        self.b = np.zeros(1)\n",
                "    \n",
                "    def forward(self, X):\n",
                "        return 1 / (1 + np.exp(-(X @ self.W + self.b)))\n",
                "    \n",
                "    def train(self, X, y, epochs=5, lr=0.1):\n",
                "        for _ in range(epochs):\n",
                "            # Simple SGD\n",
                "            preds = self.forward(X).flatten()\n",
                "            error = preds - y\n",
                "            # Gradients\n",
                "            dW = (X.T @ error.reshape(-1,1)) / len(X)\n",
                "            db = np.mean(error)\n",
                "            # Update\n",
                "            self.W -= lr * dW\n",
                "            self.b -= lr * db\n",
                "\n",
                "    def get_weights(self):\n",
                "        return self.W, self.b\n",
                "    \n",
                "    def set_weights(self, W, b):\n",
                "        self.W = W\n",
                "        self.b = b\n",
                "\n",
                "# Initialize Global Model\n",
                "global_model = SimpleNN()\n",
                "acc_history = []\n",
                "\n",
                "# --- 3. FEDERATED TRAINING LOOP ---\n",
                "rounds = 20\n",
                "for r in range(rounds):\n",
                "    # 1. Server Broadcasts Weights\n",
                "    W_global, b_global = global_model.get_weights()\n",
                "    \n",
                "    # 2. Clients Update Locally\n",
                "    # Client A\n",
                "    client_A = SimpleNN()\n",
                "    client_A.set_weights(copy.deepcopy(W_global), copy.deepcopy(b_global))\n",
                "    client_A.train(X_A, y_A, epochs=1) # Local Epochs\n",
                "    W_A, b_A = client_A.get_weights()\n",
                "    \n",
                "    # Client B\n",
                "    client_B = SimpleNN()\n",
                "    client_B.set_weights(copy.deepcopy(W_global), copy.deepcopy(b_global))\n",
                "    client_B.train(X_B, y_B, epochs=1)\n",
                "    W_B, b_B = client_B.get_weights()\n",
                "    \n",
                "    # 3. Server Aggregates (FedAvg)\n",
                "    # Weighted average based on sample size\n",
                "    n_total = len(y_A) + len(y_B)\n",
                "    W_new = (W_A * len(y_A) + W_B * len(y_B)) / n_total\n",
                "    b_new = (b_A * len(y_A) + b_B * len(y_B)) / n_total\n",
                "    \n",
                "    global_model.set_weights(W_new, b_new)\n",
                "    \n",
                "    # 4. Evaluation (On balanced Global Test Set)\n",
                "    preds = global_model.forward(X).flatten() > 0.5\n",
                "    acc = (preds == y).mean()\n",
                "    acc_history.append(acc)\n",
                "    if r % 5 == 0:\n",
                "        print(f\"Round {r}: Global Accuracy = {acc:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Convergence Analysis\n",
                "Did the model learn? FedAvg usually learns slower than Centralized Training, but preserves privacy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 4))\n",
                "plt.plot(acc_history)\n",
                "plt.title(\"Federated Learning Convergence\")\n",
                "plt.xlabel(\"Communication Round\")\n",
                "plt.ylabel(\"Global Accuracy\")\n",
                "plt.grid()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}