{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Implementation: Self-Attention from Scratch\n",
                "\n",
                "**Goal**: Implement $Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# 1. Mock Data (3 words, embedding dim 4)\n",
                "x = np.array([\n",
                "    [1, 0, 1, 0], # Word 1 (e.g., \"I\")\n",
                "    [0, 1, 0, 1], # Word 2 (e.g., \"love\")\n",
                "    [1, 1, 0, 0]  # Word 3 (e.g., \"AI\")\n",
                "])\n",
                "\n",
                "# In Self-Attention, Input = Query = Key = Value (conceptually)\n",
                "# In reality, we multiply by weight matrices Wq, Wk, Wv.\n",
                "# Here we simplify and assume W are identity matrices, so Q=K=V=x.\n",
                "Q = x\n",
                "K = x\n",
                "V = x\n",
                "\n",
                "# 2. Calculate Attention Scores (Q @ K_transpose)\n",
                "scores = np.dot(Q, K.T)\n",
                "print(\"Raw Scores (Dot Product):\\n\", scores)\n",
                "\n",
                "# 3. Softmax (Normalize to probabilities)\n",
                "def softmax(z):\n",
                "    exp_z = np.exp(z)\n",
                "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
                "\n",
                "# Scale by sqrt(d_k) for stability\n",
                "d_k = x.shape[1]\n",
                "attention_weights = softmax(scores / np.sqrt(d_k))\n",
                "print(\"\\nAttention Weights:\\n\", np.round(attention_weights, 2))\n",
                "\n",
                "# 4. Weighted Sum (Weights @ Values)\n",
                "output = np.dot(attention_weights, V)\n",
                "print(\"\\nOutput (Contextualized Embeddings):\\n\", output)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "See the \"Attention Weights\" matrix. The diagonal is highest (words attend to themselves), but they also attend to others."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}