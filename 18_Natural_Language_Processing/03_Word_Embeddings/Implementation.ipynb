{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Implementation: Using Pre-trained Embeddings\n",
                "\n",
                "**Goal**: Use Gensim to load GloVe vectors (Mocked for speed)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "\n",
                "# 1. Mock Embedding Dictionary (Real models are 3GB+)\n",
                "# Imagine these are 3D embeddings for simplicity\n",
                "embeddings = {\n",
                "    \"king\": np.array([0.9, 0.1, 0.5]),\n",
                "    \"queen\": np.array([0.9, 0.2, 0.5]), # Similar to King but slightly different\n",
                "    \"man\": np.array([0.1, 0.1, 0.2]),\n",
                "    \"woman\": np.array([0.1, 0.2, 0.2]),\n",
                "    \"apple\": np.array([-0.5, -0.5, 0])  # Completely different direction\n",
                "}\n",
                "\n",
                "def get_similarity(w1, w2):\n",
                "    v1 = embeddings[w1].reshape(1, -1)\n",
                "    v2 = embeddings[w2].reshape(1, -1)\n",
                "    return cosine_similarity(v1, v2)[0][0]\n",
                "\n",
                "# 2. Test Similarity\n",
                "print(f\"King vs Queen: {get_similarity('king', 'queen'):.4f}\")\n",
                "print(f\"King vs Apple: {get_similarity('king', 'apple'):.4f}\")\n",
                "\n",
                "# 3. The Analogy Test\n",
                "# King - Man + Woman = ?\n",
                "target = embeddings['king'] - embeddings['man'] + embeddings['woman']\n",
                "\n",
                "# Find closest match\n",
                "best_word = None\n",
                "best_score = -1\n",
                "\n",
                "for word, vec in embeddings.items():\n",
                "    if word in ['king', 'man', 'woman']: continue\n",
                "    score = cosine_similarity(target.reshape(1, -1), vec.reshape(1, -1))[0][0]\n",
                "    print(f\"Score for {word}: {score:.4f}\")\n",
                "    if score > best_score:\n",
                "        best_score = score\n",
                "        best_word = word\n",
                "        \n",
                "print(f\"\\nAnalogy Result: King - Man + Woman = {best_word.upper()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "Vectors allow us to do math with meaning."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}