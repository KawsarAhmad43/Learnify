{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Comprehensive Research: Credit Card Fraud Detection\n",
                "\n",
                "## 1. Environment & Problem Definition\n",
                "**Objective**: Identify fraudulent transactions in a highly imbalanced dataset (0.17% Positive Class).\n",
                "**Metric**: Area Under Precision-Recall Curve (AUPRC) & Recall at Precision > 0.8.\n",
                "**Research Questions**:\n",
                "1.  Does SMOTE improve the classifier or just add noise?\n",
                "2.  What is the optimal Decision Threshold for a business maximizing Recall?\n",
                "3.  Which features drive fraud: Time, Amount, or V-features?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, make_scorer\n",
                "from sklearn.preprocessing import RobustScaler\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "\n",
                "# Set plotting style for research paper quality\n",
                "sns.set(style=\"whitegrid\", context=\"notebook\", palette=\"muted\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "# --- STEP 1: DATA GENERATION (Simulating the Problem) ---\n",
                "# We generate \"Dirty\" data to demonstrate cleaning.\n",
                "def generate_research_data(n=20000):\n",
                "    n_fraud = int(n * 0.02) # 2% Fraud for visibility (Real 0.17% is harder to viz)\n",
                "    n_normal = n - n_fraud\n",
                "    \n",
                "    # Feature V1: Normal dist. Fraud is shifted.\n",
                "    v1_norm = np.random.normal(0, 1, n_normal)\n",
                "    v1_fraud = np.random.normal(3, 1.5, n_fraud) # Shifted right\n",
                "    \n",
                "    # Feature Amount: Exponential. Fraud has some high value outliers.\n",
                "    amt_norm = np.random.exponential(100, n_normal)\n",
                "    amt_fraud = np.random.exponential(150, n_fraud) + np.random.choice([0, 1000, 5000], n_fraud, p=[0.8, 0.15, 0.05])\n",
                "    \n",
                "    X = pd.DataFrame({\n",
                "        \"V1\": np.concatenate([v1_norm, v1_fraud]),\n",
                "        \"Amount\": np.concatenate([amt_norm, amt_fraud])\n",
                "    })\n",
                "    y = np.array([0]*n_normal + [1]*n_fraud)\n",
                "    \n",
                "    # Add Missing Values (Simulation)\n",
                "    mask = np.random.random(n) < 0.05 # 5% missing\n",
                "    X.loc[mask, \"V1\"] = np.nan\n",
                "    \n",
                "    return X, y\n",
                "\n",
                "X, y = generate_research_data()\n",
                "print(f\"Data Generated: {X.shape}. Fraud Ratio: {y.mean():.2%}\")\n",
                "print(\"Missing Values:\\n\", X.isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploratory Data Analysis (EDA)\n",
                "Before modeling, we must understand the distribution. \n",
                "*   **Hypothesis**: Fraud transactions involve uncommon Amounts (either very small or very large)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.1 Univariate Analysis: Amount Distribution\n",
                "plt.figure(figsize=(14, 5))\n",
                "plt.subplot(1, 2, 1)\n",
                "sns.histplot(X['Amount'], bins=50, kde=True)\n",
                "plt.title(\"Amount Distribution (Raw)\")\n",
                "\n",
                "# 2.2 Bivariate: Amount vs Class\n",
                "plt.subplot(1, 2, 2)\n",
                "sns.boxplot(x=y, y=X['Amount'])\n",
                "plt.title(\"Amount by Class (0=Normal, 1=Fraud)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation**: The 'Amount' is heavily right-skewed. The Boxplot shows extreme outliers in the Fraud class. We need to handle this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Data Cleaning & Feature Engineering\n",
                "\n",
                "# 3.1 Imputation\n",
                "print(\"Imputing V1 with Median (Robust to outliers)...\")\n",
                "X['V1'].fillna(X['V1'].median(), inplace=True)\n",
                "\n",
                "# 3.2 Log Transformation\n",
                "# Standard convention for financial data: log(1 + x)\n",
                "X['Log_Amount'] = np.log1p(X['Amount'])\n",
                "\n",
                "# Verify Improvement\n",
                "plt.figure(figsize=(8, 4))\n",
                "sns.kdeplot(data=X, x='Log_Amount', hue=y, fill=True)\n",
                "plt.title(\"Log-Amount Distribution by Class\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Modeling Strategy\n",
                "We will use **SMOTE (Synthetic Minority Over-sampling Technique)**. \n",
                "\n",
                "**Critical Research Concept**: \n",
                "SMOTE must *only* be applied to the Training sets during Cross-Validation. If we apply it to the whole dataset before splitting, information from the Test set leaks into the Train set (since SMOTE uses Nearest Neighbors), invalidating our results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 Train/Test Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X[['V1', 'Log_Amount']], y, test_size=0.2, stratify=y, random_state=42)\n",
                "\n",
                "# 4.2 Define Pipeline\n",
                "# ImbPipeline allows resampling inside the CV loop\n",
                "pipeline = ImbPipeline([\n",
                "    ('scaler', RobustScaler()), # Scale features using Median/IQR\n",
                "    ('smote', SMOTE(sampling_strategy=0.1, random_state=42)), # Bring Fraud up to 10%\n",
                "    ('clf', RandomForestClassifier(random_state=42))\n",
                "])\n",
                "\n",
                "# 4.3 Hyperparameter Tuning (GridSearch)\n",
                "# We explore: Does deep trees help? Does balanced class weight help?\n",
                "param_grid = {\n",
                "    'clf__n_estimators': [50, 100],\n",
                "    'clf__max_depth': [5, 10, None],\n",
                "    'clf__class_weight': [None, 'balanced']\n",
                "}\n",
                "\n",
                "# Custom Scorer: AUPRC is better than AUC-ROC for imbalance\n",
                "def auprc_score(y_true, y_pred_proba):\n",
                "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
                "    return auc(recall, precision)\n",
                "\n",
                "scorer = make_scorer(auprc_score, needs_proba=True)\n",
                "\n",
                "print(\"Starting Grid Search...\")\n",
                "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring=scorer, verbose=1)\n",
                "grid.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Best Parameters: {grid.best_params_}\")\n",
                "print(f\"Best CV AUPRC: {grid.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Diagnostics & Evaluation\n",
                "A simple accuracy score is not enough. We need to visualize the trade-off between Precision and Recall."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5.1 Learning Curve (Bias vs Variance Analysis)\n",
                "train_sizes, train_scores, val_scores = learning_curve(\n",
                "    grid.best_estimator_, X_train, y_train, cv=3, scoring=scorer, train_sizes=np.linspace(0.1, 1.0, 5)\n",
                ")\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label=\"Training Score\")\n",
                "plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label=\"Validation Score\")\n",
                "plt.title(\"Learning Curve: Are we Overfitting?\")\n",
                "plt.xlabel(\"Training Examples\")\n",
                "plt.ylabel(\"AUPRC Score\")\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# 5.2 Precision-Recall Curve on Test Set\n",
                "best_model = grid.best_estimator_\n",
                "probas = best_model.predict_proba(X_test)[:, 1]\n",
                "precision, recall, thresholds = precision_recall_curve(y_test, probas)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(recall, precision, label=f'Model (AP={auc(recall, precision):.2f})')\n",
                "plt.xlabel('Recall (Fraud Captured)')\n",
                "plt.ylabel('Precision (True Fraud %)')\n",
                "plt.title('Precision-Recall Curve')\n",
                "plt.legend()\n",
                "plt.grid()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Threshold Moving (Optimization)\n",
    "The default threshold is 0.5. However, for fraud, missing a fraud (False Negative) costs $10,000, while calling a customer (False Positive) costs $5. We should lower the threshold to increase Recall."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find threshold that guarantees 90% Recall\n",
                "target_recall = 0.90\n",
                "idx = np.argmin(np.abs(recall - target_recall))\n",
                "optimal_threshold = thresholds[idx]\n",
                "\n",
                "print(f\"To achieve {target_recall*100}% Recall, we need Threshold = {optimal_threshold:.4f}\")\n",
                "print(f\"At this level, Precision is {precision[idx]:.2f}\")\n",
                "\n",
                "# Final Confusion Matrix\n",
                "y_pred = (probas >= optimal_threshold).astype(int)\n",
                "print(\"\\nConfusion Matrix at Optimal Threshold:\")\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "\n",
                "# Feature Importance\n",
                "importances = best_model.named_steps['clf'].feature_importances_\n",
                "print(\"\\nFeature Importances:\")\n",
                "for name, imp in zip(['V1', 'Log_Amount'], importances):\n",
                "    print(f\"{name}: {imp:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}