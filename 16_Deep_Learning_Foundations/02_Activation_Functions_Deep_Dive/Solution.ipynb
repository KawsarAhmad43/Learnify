{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Solution: ReLU Family Comparison\n",
                "\n",
                "**Objective**: Compare ReLU, Leaky ReLU, and ELU behavior on negative inputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# 1. Define Functions\n",
                "def relu(x):\n",
                "    return np.maximum(0, x)\n",
                "\n",
                "def leaky_relu(x, alpha=0.1):\n",
                "    return np.where(x > 0, x, x * alpha)\n",
                "\n",
                "def elu(x, alpha=1.0):\n",
                "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
                "\n",
                "# 2. Generate Data\n",
                "x = np.linspace(-3, 1, 400) # Focusing on the transition around 0\n",
                "\n",
                "# 3. Plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(x, relu(x), label=\"ReLU\", linestyle=':', linewidth=3, color='black')\n",
                "plt.plot(x, leaky_relu(x), label=\"Leaky ReLU (alpha=0.1)\", color='green')\n",
                "plt.plot(x, elu(x), label=\"ELU (alpha=1.0)\", color='red')\n",
                "\n",
                "plt.title(\"The Battle of ReLUs\")\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.axhline(0, color='black', linewidth=0.5)\n",
                "plt.axvline(0, color='black', linewidth=0.5)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analysis\n",
                "*   **ReLU**: Strict 0 for negative values. Causes dead neurons.\n",
                "*   **Leaky ReLU**: Linear negative slope. Sharp turn at 0 (Not differentiable at 0).\n",
                "*   **ELU**: Curved negative log-like shape. Smooth curve at 0 (Differentiable everywhere). This smoothness can help optimization."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}