{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Implementation: Dropout from Scratch\n",
                "\n",
                "**Goal**: Implement the Dropout mechanism."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def dropout_layer(X, drop_prob=0.5, training=True):\n",
                "    if not training:\n",
                "        # During testing, we use the full network\n",
                "        return X\n",
                "    \n",
                "    # Create a mask of 0s and 1s\n",
                "    # Keep probability = 1 - drop_prob\n",
                "    keep_prob = 1 - drop_prob\n",
                "    mask = np.random.binomial(n=1, p=keep_prob, size=X.shape)\n",
                "    \n",
                "    # Apply mask\n",
                "    # IMPORTANT: We scale by (1/keep_prob) to maintain the expected sum of values\n",
                "    # This is called \"Inverted Dropout\"\n",
                "    out = (X * mask) / keep_prob\n",
                "    \n",
                "    return out, mask\n",
                "\n",
                "# Mock Information\n",
                "X = np.ones((1, 10)) # Layer with 10 neurons, all value 1\n",
                "\n",
                "print(\"Original:\", X)\n",
                "out, mask = dropout_layer(X, drop_prob=0.5)\n",
                "print(\"After Dropout:\\n\", out)\n",
                "print(\"Mask:\\n\", mask)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison\n",
                "*   **Original Sum**: 10\n",
                "*   **Dropout Sum**: roughly 10 (due to scaling)\n",
                "*   Without scaling, the next layer would receive much smaller signals during training than testing."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}