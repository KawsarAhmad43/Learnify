{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Classification Metrics Solutions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Task 1 Code verification\n",
                "TP = 50\n",
                "TN = 800\n",
                "FP = 100\n",
                "FN = 50\n",
                "\n",
                "acc = (TP+TN)/(TP+TN+FP+FN)\n",
                "prec = TP / (TP+FP)\n",
                "rec = TP / (TP+FN)\n",
                "f1 = 2 * (prec*rec)/(prec+rec)\n",
                "\n",
                "print(f\"Acc: {acc}\")\n",
                "print(f\"Precision: {prec:.2f}\")\n",
                "print(f\"Recall: {rec:.2f}\")\n",
                "print(f\"F1: {f1:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 2: Thresholds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.metrics import precision_score, recall_score\n",
                "\n",
                "# Recreating prior data\n",
                "y_true = np.array([0]*90 + [1]*10)\n",
                "np.random.seed(42)\n",
                "y_probs = np.random.rand(100) * 0.4\n",
                "y_probs[90:] += 0.5\n",
                "\n",
                "def eval_threshold(thresh):\n",
                "    y_pred_thresh = (y_probs > thresh).astype(int)\n",
                "    p = precision_score(y_true, y_pred_thresh, zero_division=0)\n",
                "    r = recall_score(y_true, y_pred_thresh)\n",
                "    print(f\"Thresh {thresh} -> Precision: {p:.2f}, Recall: {r:.2f}\")\n",
                "\n",
                "eval_threshold(0.1) # Aggressive (High Recall, Low Prec)\n",
                "eval_threshold(0.5) # Balanced\n",
                "eval_threshold(0.9) # Conservative (Low Recall, High Prec usually, but here 0 if no one found)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 3: Multiclass Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import classification_report\n",
                "y_t = [0, 1, 2, 2, 0]\n",
                "y_p = [0, 0, 2, 1, 0]\n",
                "print(classification_report(y_t, y_p))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}