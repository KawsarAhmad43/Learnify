{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Comprehensive Research: Fine-Tuning LLMs (BERT)\n",
                "\n",
                "## 1. Concept\n",
                "**Objective**: Adapt a General LLM to a Specific Task.\n",
                "**Risk**: Catastrophic Forgetting (Overwriting pre-trained knowledge).\n",
                "**Control**: Warmup Scheduler & Weight Decay.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
                "from datasets import Dataset\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# --- 1. MOCK DATASET ---\n",
                "# Simulating the IMDB dataset structure\n",
                "texts = [\"This movie was fantastic!\", \"Horrible, waste of time.\", \"Great acting but bad plot.\", \"I loved it.\"] * 50\n",
                "labels = [1, 0, 0, 1] * 50 # 200 samples\n",
                "\n",
                "df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
                "raw_ds = Dataset.from_pandas(df)\n",
                "\n",
                "print(\"Top 5 Samples:\")\n",
                "print(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Tokenization Analysis\n",
                "BERT doesn't read words; it reads Sub-words. Let's see how our data gets chopped up."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_ckpt = \"distilbert-base-uncased\"\n",
                "tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
                "\n",
                "def tokenize(batch):\n",
                "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n",
                "\n",
                "tokenized_ds = raw_ds.map(tokenize, batched=True)\n",
                "\n",
                "# Inspect Tokens\n",
                "sample_ids = tokenized_ds[0]['input_ids']\n",
                "print(f\"Raw IDs: {sample_ids}\")\n",
                "print(f\"Decoded: {tokenizer.decode(sample_ids)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation**: Note the `[CLS]` (Start) and `[SEP]` (End) tokens. These are mandatory for BERT."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Model Initialization\n",
                "model = DistilBertForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
                "\n",
                "# 4. Training Arguments (The Research Part)\n",
                "# We strictly control the learning rate to avoid destroying the pre-trained weights.\n",
                "args = TrainingArguments(\n",
                "    output_dir=\"bert-finetuned\",\n",
                "    num_train_epochs=3,\n",
                "    learning_rate=2e-5, # Very small (Standard is 1e-3, which is 50x larger)\n",
                "    per_device_train_batch_size=8,\n",
                "    weight_decay=0.01, # Regularization\n",
                "    lr_scheduler_type='linear', # Triangular warm-up\n",
                "    warmup_ratio=0.1, # 10% of steps used to ramp up LR\n",
                "    logging_steps=10\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=args,\n",
                "    train_dataset=tokenized_ds,\n",
                ")\n",
                "\n",
                "print(\"Starting Fine-Tuning...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Learning Curve Diagnostics\n",
                "We access the Trainer's logs to plot the Loss over time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logs = trainer.state.log_history\n",
                "loss = [x['loss'] for x in logs if 'loss' in x]\n",
                "steps = [x['step'] for x in logs if 'loss' in x]\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(steps, loss, label=\"Training Loss\")\n",
                "plt.title(\"Fine-Tuning Dynamics\")\n",
                "plt.xlabel(\"Steps\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.grid()\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}