# Quantum Gradients

## 1. The Challenge of Training
*   To train a QNN, we need $\nabla_\theta L$ (Gradient of Loss w.r.t parameters).
*   **Finite Differences**: $f'(x) \approx \frac{f(x+\epsilon) - f(x)}{\epsilon}$. (Numerical). Unstable on quantum hardware due to noise. e.g., $f(x)$ is noisy, so $f(x+\epsilon) - f(x)$ is mostly noise.

## 2. Parameter Shift Rule
*   For gates generated by Pauli operators (like $R_x, R_y, R_z$), the gradient is ANALYTIC.
*   $\frac{\partial f}{\partial \theta} = \frac{1}{2} [f(\theta + \frac{\pi}{2}) - f(\theta - \frac{\pi}{2})]$.
*   This is exact, not an approximation.
*   It is robust to shot noise (because we are subtracting two macroscopic shifts, not microscopic $\epsilon$).

## 3. Quantum Natural Gradient
*   Standard Gradient Descent assumes Euclidian geometry.
*   The Hilbert Space actually has **Fubini-Study Geometric** structure.
*   **Quantum Natural Gradient (QNG)** adjusts the update step based on the curvature of the quantum state space.
*   Result: Converges much faster (fewer epochs).
