{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Implementation: Tiny RAG\n",
                "\n",
                "**Goal**: Build RAG from scratch using Python lists."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def retrieve(query, database, top_k=1):\n",
                "    # 1. Simple Keyword Matching (Naive Retriever)\n",
                "    scores = []\n",
                "    query_words = set(query.lower().split())\n",
                "    \n",
                "    for doc in database:\n",
                "        doc_words = set(doc.lower().split())\n",
                "        # Jaccard Similarity\n",
                "        intersection = query_words.intersection(doc_words)\n",
                "        score = len(intersection) / len(query_words.union(doc_words))\n",
                "        scores.append((score, doc))\n",
                "    \n",
                "    # Sort desc\n",
                "    scores.sort(key=lambda x: x[0], reverse=True)\n",
                "    return [doc for score, doc in scores[:top_k]]\n",
                "\n",
                "def generate(query, context):\n",
                "    # Mock LLM\n",
                "    return f\"Based on '{context}', the answer is related to the query '{query}'.\"\n",
                "\n",
                "# 1. Database\n",
                "db = [\n",
                "    \"The CEO of Acme Corp is Jane Doe.\",\n",
                "    \"The company revenue was $1M last year.\",\n",
                "    \"Apples are red.\"\n",
                "]\n",
                "\n",
                "# 2. Query\n",
                "user_q = \"Who is CEO of Acme?\"\n",
                "\n",
                "# 3. RAG Pipeline\n",
                "retrieved_docs = retrieve(user_q, db)\n",
                "context_str = \"\\n\".join(retrieved_docs)\n",
                "answer = generate(user_q, context_str)\n",
                "\n",
                "print(f\"Retrieval: {retrieved_docs}\")\n",
                "print(f\"LLM Answer: {answer}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "Even simple keyword search can ground an LLM."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}